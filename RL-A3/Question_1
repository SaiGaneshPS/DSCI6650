import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random

class GridWorld:
    def __init__(self):
        
        # Definition of the given grid
        # with the rewards for the transitions
        self.grid = np.array([
            [ 0,  -1,  -1,  -1, 0],
            [ -1, -1, -1, -1,  -1],
            [ -20, -20, -1,  -20, -20],
            [ -1, -1, -1, -1, -1],
            [ -1,  -1,  -1,  -1,  -1],
        ])

        # Location of start (Bottom left corner)
        self.start_state = (4, 0)
        
        # Location of Black squares
        self.terminal_states = [(0, 0), (0, 4)]
        self.state = self.start_state

        self.first_move = True

    # 0:Up, 1:Right, 2:Down, 3:Left
    def step(self, action):
        if self.first_move:
            # First move made with equal probability
            possible_actions = [0, 1, 2, 3]
            action = random.choice(possible_actions)
            self.first_move = False

        r, c = self.state

        if action == 0:
            r -= 1
        elif action == 1:
            c += 1
        elif action == 2:
            r += 1
        elif action == 3:
            c -= 1
        
        # Reward of -1 for trying to go out of bounds
        if r < 0 or r >= self.grid.shape[0] or c < 0 or c >= self.grid.shape[1]:
            r, c = self.state
            reward = -1
        else:
            reward = self.grid[r, c]
            if reward == -20:
                # Go back to start state if current square is red
                r, c = self.start_state
        
        self.state = (r, c)
        done = self.state in self.terminal_states
        
        return self.state, reward, done

    def reset(self):
        self.state = self.start_state
        return self.state
    
# Epsilon greedy policy
def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(4)
    else:
        return np.argmax(Q[state[0], state[1]])

# SARSA Algorithm
def sarsa(env, episodes, alpha, gamma, epsilon):
    Q = np.zeros((env.grid.shape[0], env.grid.shape[1], 4))
    episode_rewards = []

    for _ in range(episodes):
        state = env.reset()
        action = epsilon_greedy(Q, state, epsilon)
        done = False
        total_reward = 0

        while not done:
            next_state, reward, done = env.step(action)
            next_action = epsilon_greedy(Q, next_state, epsilon)

            # Q-value update
            Q[state[0], state[1], action] += alpha * (
                reward + gamma * Q[next_state[0], next_state[1], next_action] -
                Q[state[0], state[1], action]
            )
            state = next_state
            action = next_action
            total_reward += reward

        episode_rewards.append(total_reward)

    return Q, episode_rewards

# Q Learning Algorithm
def q_learning(env, episodes, alpha, gamma, epsilon):
    Q = np.zeros((env.grid.shape[0], env.grid.shape[1], 4))
    episode_rewards = []

    for _ in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0

        while not done:
            action = epsilon_greedy(Q, state, epsilon)
            next_state, reward, done = env.step(action)

            # Q-value update
            Q[state[0], state[1], action] += alpha * (
                reward + gamma * np.max(Q[next_state[0], next_state[1]]) -
                Q[state[0], state[1], action]
            )
            state = next_state
            total_reward += reward

        episode_rewards.append(total_reward)

    return Q, episode_rewards

# Function to plot trajectory of the respective algorithm
def plot_trajectory(env, Q, title):
    state = env.reset()
    trajectory = [state]
    done = False
    while not done:
        action = np.argmax(Q[state[0], state[1]])
        next_state, reward, done = env.step(action)
        trajectory.append(next_state)
        state = next_state

    # Create a grid to visualize the trajectory
    grid = np.zeros(env.grid.shape)
    for i, (r, c) in enumerate(trajectory):
        grid[r, c] = i + 1

    plt.figure(figsize=(8, 8))
    sns.heatmap(grid, annot=True, cmap="coolwarm", cbar=False, square=True)
    plt.title(title)
    plt.show()

def plot_rewards(episode_rewards, title):
    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards, label='Sum of Rewards per Episode')
    plt.xlabel('Episode')
    plt.ylabel('Sum of Rewards')
    plt.title(title)
    plt.legend()
    plt.show()
    
# Defining the environment and finding the trajectories
env = GridWorld()
episodes = 10000
alpha = 0.01
gamma = 0.9
epsilon = 0.2

Q_sarsa, rewards_sarsa = sarsa(env, episodes, alpha, gamma, epsilon)
Q_q_learning, rewards_q_learning = q_learning(env, episodes, alpha, gamma, epsilon)

plot_trajectory(env, Q_sarsa, "Trajectory using Sarsa")
plot_trajectory(env, Q_q_learning, "Trajectory using Q-learning")

plot_rewards(rewards_sarsa, "SARSA: Sum of Rewards per Episode")
plot_rewards(rewards_q_learning, "Q-learning: Sum of Rewards per Episode")
