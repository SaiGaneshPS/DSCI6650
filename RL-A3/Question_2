import numpy as np
import matplotlib.pyplot as plt

class GridWorld:
    def __init__(self):

        # 7x7 Gridworld with the start state at the center
        self.grid_size = 7
        self.start_state = (3, 3)
        self.terminal_states = {(6, 0): -1, (0, 6): 1}
        self.state = self.start_state

    # 0:Up, 1:Right, 2:Down, 3:Left
    def step(self, action):
        r, c = self.state
        if action == 0:
            r -= 1
        elif action == 1:
            c += 1
        elif action == 2:
            r += 1
        elif action == 3:
            c -= 1
        
        # Zero reward for trying to move out of bounds
        if r < 0 or r >= self.grid_size or c < 0 or c >= self.grid_size:
            r, c = self.state
            reward = 0
        else:
            self.state = (r, c)
            reward = self.terminal_states.get(self.state, 0)
        
        done = self.state in self.terminal_states
        return self.state, reward, done

    def reset(self):
        self.state = self.start_state
        return self.state

# Gradient monte carlo function
def gradient_monte_carlo(env, episodes, alpha=0.01):
    grid_size = env.grid_size
    num_features = grid_size * grid_size
    w = np.zeros(num_features)
    
    def get_features(state):
        features = np.zeros(num_features)
        features[state[0] * grid_size + state[1]] = 1
        return features

    def value_function(state):
        return np.dot(w, get_features(state))

    for episode in range(episodes):
        state = env.reset()
        states = []
        rewards = []
        done = False
        
        while not done:
            # Equiprobable Random action
            action = np.random.choice([0, 1, 2, 3])
            next_state, reward, done = env.step(action)
            states.append(next_state)
            rewards.append(reward)
            state = next_state
        
        G = 0
        for t in reversed(range(len(rewards))):
            G = rewards[t] + G
            state = states[t]
            features = get_features(state)
            prediction = value_function(state)

            # Gradient Monte Carlo weight update
            w += alpha * (G - prediction) * features

    # Compute value function from weights
    V = np.zeros((grid_size, grid_size))
    for i in range(grid_size):
        for j in range(grid_size):
            V[i, j] = value_function((i, j))
    
    return V

# Semi-Gradient TD(0) Method with Affine Function Approximation
class LinearFunctionApproximation:
    def __init__(self, grid_size):
        self.grid_size = grid_size
        
        # Prevention of large weights by multiplying with 0.01 and adding regularization
        self.weights = np.random.randn(5) * 0.01
        self.regularization_term = 0.001

    def features(self, state):
        r, c = state
        # Scaling the features
        r_scaled = r / (self.grid_size - 1)
        c_scaled = c / (self.grid_size - 1)
        return np.array([1, r_scaled, c_scaled, r_scaled**2, c_scaled**2])

    def predict(self, state):
        prediction = np.dot(self.features(state), self.weights)
        return prediction

    def update(self, state, target, alpha):
        prediction = self.predict(state)
        features = self.features(state)
        td_error = target - prediction

        # Using regularization to update weights
        self.weights += alpha * (td_error * features - self.regularization_term * self.weights)

def td0(env, episodes, alpha=0.01, gamma=0.9):
    V = LinearFunctionApproximation(env.grid_size)
    for episode in range(episodes):
        state = env.reset()
        done = False
        
        while not done:
            action = np.random.choice([0, 1, 2, 3])
            next_state, reward, done = env.step(action)
            target = reward + (0 if done else gamma * V.predict(next_state))
            V.update(state, target, alpha)
            state = next_state

    return V

# Finding the exact value function for comparison
def exact_value_function(grid_size=7, theta=1e-5):
    V_exact = np.zeros((grid_size, grid_size))
    V_exact[6, 0] = -1
    V_exact[0, 6] = 1
    delta = theta

    while delta >= theta:
        delta = 0
        for r in range(grid_size):
            for c in range(grid_size):
                if (r, c) in [(6, 0), (0, 6)]:
                    continue
                
                v = V_exact[r, c]
                neighbors = [(max(r-1, 0), c), (min(r+1, grid_size-1), c), 
                             (r, max(c-1, 0)), (r, min(c+1, grid_size-1))]
                V_exact[r, c] = 0.25 * sum([V_exact[nr, nc] for nr, nc in neighbors])
                delta = max(delta, abs(v - V_exact[r, c]))

    return V_exact

env = GridWorld()
episodes = 50000
alpha = 0.01
gamma = 0.9

V_exact = exact_value_function()

# Gradient monte carlo run
V_mc = gradient_monte_carlo(env, episodes, alpha=alpha)

# Semi Gradient TD(0) run
V_td = td0(env, episodes, alpha=alpha, gamma=gamma)

# Plotting the respective value functions
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(V_mc, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.title("Gradient Monte Carlo Value Function")

V_td_values = np.array([[V_td.predict((r, c)) for c in range(env.grid_size)] for r in range(env.grid_size)])
plt.subplot(1, 2, 2)
plt.imshow(V_td_values, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.title("TD(0) Value Function with Affine Approximation")

plt.show()

# Finding mean square error between value functions
def mse(V1, V2):
    return np.mean((V1 - V2) ** 2)

# Calculating the MSE
mse_mc = mse(V_mc, V_exact)
mse_td = mse(V_td_values, V_exact)

# Printing the MSEs
print(f"MSE between Gradient Monte Carlo and Exact Value Function: {mse_mc}")
print(f"MSE between Semi-Gradient TD(0) and Exact Value Function: {mse_td}")

# Comparing all value functions
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.imshow(V_exact, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.title("Exact Value Function")

plt.subplot(1, 3, 2)
plt.imshow(V_mc, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.title("Gradient Monte Carlo Value Function")

plt.subplot(1, 3, 3)
plt.imshow(V_td_values, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.title("TD(0) Value Function")

plt.show()
