import numpy as np
import matplotlib.pyplot as plt

# A Simple K-Armed bandit problem with stationary reward distributions
def bandit_problem(k=10):
    means = np.random.normal(0, 1, k)
    return means

# Obtain reward from a normal distribution when given a mean and variance value
def get_reward(mean):
    return np.random.normal(mean, 1)

# Greedy algorithm uses epsilon value as 0
# While epislon-greedy algorithm will have a non-zero epsilon value
def greedy(k, steps, runs, epsilon=0):
    rewards = np.zeros((runs, steps))
    optimal_actions = np.zeros((runs, steps))
    
    for run in range(runs):
        means = bandit_problem(k)
        q_values = np.zeros(k)
        action_counts = np.zeros(k)
        
        for step in range(steps):
            if np.random.rand() < epsilon:
                action = np.random.choice(k)
            else:
                action = np.argmax(q_values)
            
            reward = get_reward(means[action])
            rewards[run, step] = reward
            optimal_actions[run, step] = action == np.argmax(means)
            
            action_counts[action] += 1
            q_values[action] += (reward - q_values[action]) / action_counts[action]
    
    return rewards, optimal_actions

# Greedey algorithm but it uses an optimistic q-value in the initial step
def optimistic_greedy(k, steps, runs, initial_value):
    rewards = np.zeros((runs, steps))
    optimal_actions = np.zeros((runs, steps))
    
    for run in range(runs):
        means = bandit_problem(k)
        q_values = np.full(k, initial_value)
        action_counts = np.zeros(k)
        
        for step in range(steps):
            action = np.argmax(q_values)
            reward = get_reward(means[action])
            rewards[run, step] = reward
            optimal_actions[run, step] = action == np.argmax(means)
            
            action_counts[action] += 1
            q_values[action] += (reward - q_values[action]) / action_counts[action]
    
    return rewards, optimal_actions

# This algorithm uses a learning rate for the gradient approach
def gradient_bandit(k, steps, runs, alpha):
    rewards = np.zeros((runs, steps))
    optimal_actions = np.zeros((runs, steps))
    
    for run in range(runs):
        means = bandit_problem(k)
        h = np.zeros(k)
        pi = np.ones(k) / k
        
        for step in range(steps):
            action = np.random.choice(k, p=pi)
            reward = get_reward(means[action])
            rewards[run, step] = reward
            optimal_actions[run, step] = action == np.argmax(means)
            
            avg_reward = np.mean(rewards[run, :step+1])
            h[action] += alpha * (reward - avg_reward) * (1 - pi[action])
            for a in range(k):
                if a != action:
                    h[a] -= alpha * (reward - avg_reward) * pi[a]
            
            pi = np.exp(h) / np.sum(np.exp(h))
    
    return optimal_actions

# Parameters for pilot runs
k = 10
steps = 1000

# Using fewer runs for pilot to speed up
runs = 200

# Pilot runs with different epsilon values
pilot_epsilons = [0.01, 0.1, 0.05, 0.2, 0.5]
pilot_rewards_eps = []
pilot_optimal_eps = []

for epsilon in pilot_epsilons:
    rewards, optimal_actions = greedy(k, steps, runs, epsilon)
    pilot_rewards_eps.append(np.mean(rewards, axis=0))
    pilot_optimal_eps.append(np.mean(optimal_actions, axis=0))

best_epsilon_index = np.argmax([rewards[-1] for rewards in pilot_rewards_eps])
best_epsilon = pilot_epsilons[best_epsilon_index]
print(f"Best epsilon value based on pilot runs: {best_epsilon}")

# Pilot runs with different alpha values for gradient bandit
pilot_alphas = [0.1, 0.01, 0.001, 0.05, 0.2]
pilot_optimal_alpha = []

for alpha in pilot_alphas:
    optimal_actions = gradient_bandit(k, steps, runs, alpha)
    pilot_optimal_alpha.append(np.mean(optimal_actions, axis=0))

best_alpha_index = np.argmax([optimal[-1] for optimal in pilot_optimal_alpha])
best_alpha = pilot_alphas[best_alpha_index]
print(f"Best alpha value based on pilot runs: {best_alpha}")

# Parameters
steps = 1000
runs = 1000
initial_value = 5.0

# Run algorithms with the best parameters found from pilot runs
greedy_rewards, greedy_optimal = greedy(k, steps, runs)
epsilon_greedy_rewards, epsilon_greedy_optimal = greedy(k, steps, runs, best_epsilon)
optimistic_rewards, optimistic_optimal = optimistic_greedy(k, steps, runs, initial_value)
gradient_optimal = gradient_bandit(k, steps, runs, best_alpha)

# Plot results
plt.figure(figsize=(12, 8))

# Average rewards
plt.subplot(2, 1, 1)
plt.plot(np.mean(greedy_rewards, axis=0), label="Greedy")
plt.plot(np.mean(epsilon_greedy_rewards, axis=0), label=f"Epsilon-greedy (eps={best_epsilon})")
plt.plot(np.mean(optimistic_rewards, axis=0), label="Optimistic Greedy")
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.legend()

# Optimal action percentages
plt.subplot(2, 1, 2)
plt.plot(np.mean(greedy_optimal, axis=0), label="Greedy")
plt.plot(np.mean(epsilon_greedy_optimal, axis=0), label=f"Epsilon-greedy (eps={best_epsilon})")
plt.plot(np.mean(optimistic_optimal, axis=0), label="Optimistic Greedy")
plt.plot(np.mean(gradient_optimal, axis=0), label=f"Gradient Bandit (alpha={best_alpha})")
plt.xlabel("Steps")
plt.ylabel("Optimal Action %")
plt.legend()

plt.show()
